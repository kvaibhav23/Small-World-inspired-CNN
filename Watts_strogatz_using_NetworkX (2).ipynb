{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class DAGConverter:\n",
        "    \"\"\"Convert undirected graph to Directed Acyclic Graph\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def convert_to_dag(self, graph: nx.Graph) -> nx.DiGraph:\n",
        "        \"\"\"\n",
        "        Convert undirected graph to DAG by assigning indices and directing edges\n",
        "        from smaller to larger index (as described in paper)\n",
        "        \"\"\"\n",
        "        # Create directed graph\n",
        "        dag = nx.DiGraph()\n",
        "\n",
        "        # Add all nodes\n",
        "        dag.add_nodes_from(graph.nodes())\n",
        "\n",
        "        # Add directed edges from smaller to larger index\n",
        "        for u, v in graph.edges():\n",
        "            if u < v:\n",
        "                dag.add_edge(u, v)\n",
        "            else:\n",
        "                dag.add_edge(v, u)\n",
        "\n",
        "        return dag\n",
        "\n",
        "class NodeOperation(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network node operation as described in paper\n",
        "    Performs: Aggregation -> Transformation -> Distribution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, max_inputs: int = 8, use_gelu: bool = False):\n",
        "        super(NodeOperation, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.max_inputs = max_inputs\n",
        "\n",
        "        # Aggregation weights (learnable and positive via sigmoid)\n",
        "        # Increased max_inputs to handle more connections\n",
        "        self.aggregation_weights = nn.Parameter(torch.randn(max_inputs))\n",
        "\n",
        "        # Transformation operations\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Activation function (GeLU for first modules, ReLU for others as per paper)\n",
        "        if use_gelu:\n",
        "            self.activation = nn.GELU()\n",
        "        else:\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass implementing the three steps from paper:\n",
        "        1. Aggregation: weighted sum of inputs\n",
        "        2. Transformation: conv + batch_norm + activation\n",
        "        3. Distribution: output to next nodes\n",
        "        \"\"\"\n",
        "        if not inputs:\n",
        "            raise ValueError(\"Node must have at least one input\")\n",
        "\n",
        "        # Step 1: Aggregation with learnable weights\n",
        "        if len(inputs) == 1:\n",
        "            aggregated = inputs[0]\n",
        "        else:\n",
        "            # Limit the number of inputs to max_inputs to prevent index errors\n",
        "            num_inputs = min(len(inputs), self.max_inputs)\n",
        "            limited_inputs = inputs[:num_inputs]\n",
        "\n",
        "            # Apply sigmoid to ensure positive weights\n",
        "            weights = torch.sigmoid(self.aggregation_weights[:num_inputs])\n",
        "            weights = weights / weights.sum()  # Normalize weights\n",
        "\n",
        "            # Weighted sum of inputs\n",
        "            aggregated = torch.zeros_like(limited_inputs[0])\n",
        "            for i, inp in enumerate(limited_inputs):\n",
        "                aggregated += weights[i] * inp\n",
        "\n",
        "        # Step 2: Transformation\n",
        "        x = self.conv(aggregated)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CNNBCNModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Single CNNBCN module generated from DAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dag: nx.DiGraph, in_channels: int, out_channels: int, use_gelu: bool = False):\n",
        "        super(CNNBCNModule, self).__init__()\n",
        "        self.dag = dag\n",
        "        self.nodes = list(dag.nodes())\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Calculate max inputs for any node to size the aggregation weights properly\n",
        "        max_inputs = max([dag.in_degree(node) for node in self.nodes] + [1])\n",
        "        max_inputs = max(max_inputs, 8)  # Ensure at least 8 for safety\n",
        "\n",
        "        # Create node operations\n",
        "        self.node_ops = nn.ModuleDict()\n",
        "        for node in self.nodes:\n",
        "            self.node_ops[str(node)] = NodeOperation(in_channels, out_channels, max_inputs, use_gelu)\n",
        "\n",
        "        # Input and output node mappings\n",
        "        self.input_nodes = [node for node in self.nodes if dag.in_degree(node) == 0]\n",
        "        self.output_nodes = [node for node in self.nodes if dag.out_degree(node) == 0]\n",
        "\n",
        "        # If no natural input/output nodes, use first and last\n",
        "        if not self.input_nodes:\n",
        "            self.input_nodes = [min(self.nodes)]\n",
        "        if not self.output_nodes:\n",
        "            self.output_nodes = [max(self.nodes)]\n",
        "\n",
        "        # Input projection to distribute input to input nodes\n",
        "        self.input_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        # Output aggregation\n",
        "        if len(self.output_nodes) > 1:\n",
        "            self.output_aggregation = nn.Conv2d(out_channels * len(self.output_nodes), out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.output_aggregation = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Project input\n",
        "        x_proj = self.input_projection(x)\n",
        "\n",
        "        # Initialize node outputs\n",
        "        node_outputs = {}\n",
        "\n",
        "        # Topological sort for processing order\n",
        "        topo_order = list(nx.topological_sort(self.dag))\n",
        "\n",
        "        # Process nodes in topological order\n",
        "        for node in topo_order:\n",
        "            if node in self.input_nodes:\n",
        "                # Input nodes receive the projected input\n",
        "                node_inputs = [x_proj]\n",
        "            else:\n",
        "                # Collect inputs from predecessor nodes\n",
        "                predecessors = list(self.dag.predecessors(node))\n",
        "                node_inputs = [node_outputs[pred] for pred in predecessors if pred in node_outputs]\n",
        "\n",
        "            if node_inputs:\n",
        "                node_outputs[node] = self.node_ops[str(node)](node_inputs)\n",
        "\n",
        "        # Aggregate outputs from output nodes\n",
        "        output_tensors = [node_outputs[node] for node in self.output_nodes if node in node_outputs]\n",
        "\n",
        "        if not output_tensors:\n",
        "            # Fallback: use last computed node output\n",
        "            output_tensors = [list(node_outputs.values())[-1]]\n",
        "\n",
        "        if len(output_tensors) == 1:\n",
        "            return output_tensors[0]\n",
        "        else:\n",
        "            # Concatenate and aggregate multiple outputs\n",
        "            concatenated = torch.cat(output_tensors, dim=1)\n",
        "            return self.output_aggregation(concatenated)\n",
        "\n",
        "class CNNBCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete CNNBCN model with multiple modules using NetworkX Watts-Strogatz\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, num_modules: int = 4, nodes_per_module: int = 36):\n",
        "        super(CNNBCN, self).__init__()\n",
        "\n",
        "        self.num_modules = num_modules\n",
        "        self.nodes_per_module = nodes_per_module\n",
        "\n",
        "        # Generate random graphs and convert to DAGs for each module\n",
        "        self.dags = []\n",
        "        for i in range(num_modules):\n",
        "            # Use NetworkX's watts_strogatz_graph\n",
        "            # Parameters: n=nodes, k=nearest neighbors, p=rewiring probability\n",
        "            # WS parameters as specified in paper: WS(Z=4, P=0.75)\n",
        "            ws_graph = nx.watts_strogatz_graph(n=nodes_per_module, k=4, p=0.75, seed=42+i)\n",
        "\n",
        "            # Convert to DAG\n",
        "            dag_converter = DAGConverter()\n",
        "            dag = dag_converter.convert_to_dag(ws_graph)\n",
        "            self.dags.append(dag)\n",
        "\n",
        "        # Channel progression (simple mode from paper: 78 channels)\n",
        "        # For CIFAR-10, we'll use a smaller progression\n",
        "        channels = [32, 64, 128, 256]  # Reduced from paper's 78 channels for efficiency\n",
        "\n",
        "        # Input layer\n",
        "        self.input_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1)\n",
        "        self.input_bn = nn.BatchNorm2d(channels[0])\n",
        "        self.input_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # CNNBCN modules\n",
        "        self.modules_list = nn.ModuleList()\n",
        "        for i in range(num_modules):\n",
        "            in_ch = channels[i] if i < len(channels) else channels[-1]\n",
        "            out_ch = channels[i] if i < len(channels) else channels[-1]\n",
        "\n",
        "            # Use GeLU for first two modules, ReLU for others (as per paper)\n",
        "            use_gelu = i < 2\n",
        "\n",
        "            module = CNNBCNModule(self.dags[i], in_ch, out_ch, use_gelu)\n",
        "            self.modules_list.append(module)\n",
        "\n",
        "        # Downsampling layers between modules\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        for i in range(num_modules - 1):\n",
        "            if i < len(channels) - 1:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(channels[i+1]),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            else:\n",
        "                # If we have more modules than channel sizes, use identity or simple conv\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(channels[-1], channels[-1], kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(channels[-1]),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            self.downsample_layers.append(downsample)\n",
        "\n",
        "        # Global average pooling and classifier\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(channels[-1], num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Input processing\n",
        "        x = self.input_conv(x)\n",
        "        x = self.input_bn(x)\n",
        "        x = self.input_relu(x)\n",
        "\n",
        "        # Pass through CNNBCN modules\n",
        "        for i, module in enumerate(self.modules_list):\n",
        "            x = module(x)\n",
        "\n",
        "            # Downsample between modules (except last)\n",
        "            if i < len(self.downsample_layers):\n",
        "                x = self.downsample_layers[i](x)\n",
        "\n",
        "        # Global average pooling and classification\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def get_cifar10_dataloaders(batch_size: int = 64, num_workers: int = 2):\n",
        "    \"\"\"Get CIFAR-10 data loaders\"\"\"\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs=15, lr=0.001):\n",
        "    \"\"\"Train the CNNBCN model\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total_train += target.size(0)\n",
        "            correct_train += predicted.eq(target).sum().item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}, '\n",
        "                      f'Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate training metrics\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct_train / total_train\n",
        "\n",
        "        # Test phase\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                _, predicted = output.max(1)\n",
        "                total_test += target.size(0)\n",
        "                correct_test += predicted.eq(target).sum().item()\n",
        "\n",
        "        test_acc = 100. * correct_test / total_test\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Test Acc: {test_acc:.2f}%')\n",
        "        print('-' * 50)\n",
        "\n",
        "    return train_losses, train_accuracies, test_accuracies\n",
        "\n",
        "def visualize_watts_strogatz_graph(n=32, k=4, p=0.75, seed=42):\n",
        "    \"\"\"Visualize the NetworkX Watts-Strogatz graph\"\"\"\n",
        "\n",
        "    # Generate the graph\n",
        "    ws_graph = nx.watts_strogatz_graph(n=n, k=k, p=p, seed=seed)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot 1: Original graph\n",
        "    plt.subplot(1, 2, 1)\n",
        "    pos = nx.circular_layout(ws_graph)  # Circular layout to show small-world structure\n",
        "    nx.draw(ws_graph, pos, with_labels=True, node_color='lightblue',\n",
        "            node_size=300, font_size=8, font_weight='bold')\n",
        "    plt.title(f\"WS Small-World Network\\nNodes: {n}, k: {k}, p: {p}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot 2: Convert to DAG and visualize\n",
        "    plt.subplot(1, 2, 2)\n",
        "    dag_converter = DAGConverter()\n",
        "    dag = dag_converter.convert_to_dag(ws_graph)\n",
        "\n",
        "    # Use hierarchical layout for DAG\n",
        "    pos_dag = nx.spring_layout(dag, k=1, iterations=50)\n",
        "    nx.draw(dag, pos_dag, with_labels=True, node_color='lightcoral',\n",
        "            node_size=300, font_size=8, font_weight='bold',\n",
        "            arrows=True, arrowsize=10, arrowstyle='->')\n",
        "    plt.title(f\"Converted DAG\\nNodes: {dag.number_of_nodes()}, Edges: {dag.number_of_edges()}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print graph statistics\n",
        "    print(f\"Watts-Strogatz Graph Statistics:\")\n",
        "    print(f\"  Nodes: {ws_graph.number_of_nodes()}\")\n",
        "    print(f\"  Edges: {ws_graph.number_of_edges()}\")\n",
        "    print(f\"  Average degree: {2 * ws_graph.number_of_edges() / ws_graph.number_of_nodes():.2f}\")\n",
        "    print(f\"  Clustering coefficient: {nx.average_clustering(ws_graph):.4f}\")\n",
        "\n",
        "    # Only compute path length if graph is connected\n",
        "    if nx.is_connected(ws_graph):\n",
        "        print(f\"  Average shortest path length: {nx.average_shortest_path_length(ws_graph):.4f}\")\n",
        "    else:\n",
        "        print(f\"  Graph is not connected - cannot compute average path length\")\n",
        "\n",
        "def analyze_dag_properties(dags):\n",
        "    \"\"\"Analyze properties of generated DAGs\"\"\"\n",
        "    print(\"\\nDAG Analysis:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    for i, dag in enumerate(dags):\n",
        "        print(f\"\\nModule {i+1} DAG:\")\n",
        "        print(f\"  Nodes: {dag.number_of_nodes()}\")\n",
        "        print(f\"  Edges: {dag.number_of_edges()}\")\n",
        "\n",
        "        # Calculate in-degree and out-degree statistics\n",
        "        in_degrees = [dag.in_degree(node) for node in dag.nodes()]\n",
        "        out_degrees = [dag.out_degree(node) for node in dag.nodes()]\n",
        "\n",
        "        print(f\"  Max in-degree: {max(in_degrees)}\")\n",
        "        print(f\"  Max out-degree: {max(out_degrees)}\")\n",
        "        print(f\"  Avg in-degree: {np.mean(in_degrees):.2f}\")\n",
        "        print(f\"  Avg out-degree: {np.mean(out_degrees):.2f}\")\n",
        "\n",
        "        # Count input and output nodes\n",
        "        input_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]\n",
        "        output_nodes = [node for node in dag.nodes() if dag.out_degree(node) == 0]\n",
        "\n",
        "        print(f\"  Input nodes: {len(input_nodes)}\")\n",
        "        print(f\"  Output nodes: {len(output_nodes)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate CNNBCN model with NetworkX Watts-Strogatz\"\"\"\n",
        "\n",
        "    print(\"CNNBCN Model Implementation for CIFAR-10\")\n",
        "    print(\"Using NetworkX Watts-Strogatz Graph Generator\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. Demonstrate NetworkX Watts-Strogatz Graph Generation\n",
        "    print(\"\\n1. Generating NetworkX Watts-Strogatz Small-World Network...\")\n",
        "\n",
        "    # Visualize the generated graph (optional, comment out if no display available)\n",
        "    # visualize_watts_strogatz_graph(n=32, k=4, p=0.75, seed=42)\n",
        "\n",
        "    # 2. Create and display model architecture\n",
        "    print(\"\\n2. Creating CNNBCN Model...\")\n",
        "    model = CNNBCN(num_classes=10, num_modules=4, nodes_per_module=36)\n",
        "\n",
        "    # Analyze DAG properties\n",
        "    analyze_dag_properties(model.dags)\n",
        "\n",
        "    # Print model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nModel Architecture:\")\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Number of modules: {model.num_modules}\")\n",
        "    print(f\"  Nodes per module: {model.nodes_per_module}\")\n",
        "\n",
        "    # 3. Test with dummy input\n",
        "    print(\"\\n3. Testing model with dummy input...\")\n",
        "    dummy_input = torch.randn(2, 3, 32, 32)  # Batch size 2, RGB, 32x32\n",
        "    with torch.no_grad():\n",
        "        output = model(dummy_input)\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"Model forward pass successful!\")\n",
        "\n",
        "    # 4. Load CIFAR-10 data\n",
        "    print(\"\\n4. Loading CIFAR-10 dataset...\")\n",
        "    train_loader, test_loader = get_cifar10_dataloaders(batch_size=64)\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "    # 5. Train the model (reduced epochs for demonstration)\n",
        "    print(\"\\n5. Training CNNBCN model...\")\n",
        "    train_losses, train_accs, test_accs = train_model(\n",
        "        model, train_loader, test_loader, num_epochs=15, lr=0.001\n",
        "    )\n",
        "\n",
        "    # 6. Display final results\n",
        "    print(\"\\n6. Final Results:\")\n",
        "    print(f\"Best training accuracy: {max(train_accs):.2f}%\")\n",
        "    print(f\"Best test accuracy: {max(test_accs):.2f}%\")\n",
        "    print(f\"Final test accuracy: {test_accs[-1]:.2f}%\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb1rvxnAQzWS",
        "outputId": "1c701723-01df-4347-80b5-db4fb83ef0b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNBCN Model Implementation for CIFAR-10\n",
            "Using NetworkX Watts-Strogatz Graph Generator\n",
            "==================================================\n",
            "\n",
            "1. Generating NetworkX Watts-Strogatz Small-World Network...\n",
            "\n",
            "2. Creating CNNBCN Model...\n",
            "\n",
            "DAG Analysis:\n",
            "==============================\n",
            "\n",
            "Module 1 DAG:\n",
            "  Nodes: 36\n",
            "  Edges: 72\n",
            "  Max in-degree: 7\n",
            "  Max out-degree: 6\n",
            "  Avg in-degree: 2.00\n",
            "  Avg out-degree: 2.00\n",
            "  Input nodes: 7\n",
            "  Output nodes: 4\n",
            "\n",
            "Module 2 DAG:\n",
            "  Nodes: 36\n",
            "  Edges: 72\n",
            "  Max in-degree: 5\n",
            "  Max out-degree: 5\n",
            "  Avg in-degree: 2.00\n",
            "  Avg out-degree: 2.00\n",
            "  Input nodes: 5\n",
            "  Output nodes: 4\n",
            "\n",
            "Module 3 DAG:\n",
            "  Nodes: 36\n",
            "  Edges: 72\n",
            "  Max in-degree: 5\n",
            "  Max out-degree: 6\n",
            "  Avg in-degree: 2.00\n",
            "  Avg out-degree: 2.00\n",
            "  Input nodes: 6\n",
            "  Output nodes: 4\n",
            "\n",
            "Module 4 DAG:\n",
            "  Nodes: 36\n",
            "  Edges: 72\n",
            "  Max in-degree: 7\n",
            "  Max out-degree: 7\n",
            "  Avg in-degree: 2.00\n",
            "  Avg out-degree: 2.00\n",
            "  Input nodes: 8\n",
            "  Output nodes: 7\n",
            "\n",
            "Model Architecture:\n",
            "  Total parameters: 29,278,666\n",
            "  Trainable parameters: 29,278,666\n",
            "  Number of modules: 4\n",
            "  Nodes per module: 36\n",
            "\n",
            "3. Testing model with dummy input...\n",
            "Input shape: torch.Size([2, 3, 32, 32])\n",
            "Output shape: torch.Size([2, 10])\n",
            "Model forward pass successful!\n",
            "\n",
            "4. Loading CIFAR-10 dataset...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batches: 782\n",
            "Test batches: 157\n",
            "\n",
            "5. Training CNNBCN model...\n",
            "Training on device: cuda\n",
            "Model parameters: 29,278,666\n",
            "Epoch: 1/15, Batch: 0, Loss: 2.2908\n",
            "Epoch: 1/15, Batch: 100, Loss: 2.0125\n",
            "Epoch: 1/15, Batch: 200, Loss: 1.9656\n",
            "Epoch: 1/15, Batch: 300, Loss: 1.9773\n",
            "Epoch: 1/15, Batch: 400, Loss: 1.8446\n",
            "Epoch: 1/15, Batch: 500, Loss: 1.8207\n",
            "Epoch: 1/15, Batch: 600, Loss: 1.9157\n",
            "Epoch: 1/15, Batch: 700, Loss: 1.7288\n",
            "Epoch 1/15:\n",
            "  Train Loss: 1.9322, Train Acc: 24.05%\n",
            "  Test Acc: 32.49%\n",
            "--------------------------------------------------\n",
            "Epoch: 2/15, Batch: 0, Loss: 1.7979\n",
            "Epoch: 2/15, Batch: 100, Loss: 1.6052\n",
            "Epoch: 2/15, Batch: 200, Loss: 1.6842\n",
            "Epoch: 2/15, Batch: 300, Loss: 1.7130\n",
            "Epoch: 2/15, Batch: 400, Loss: 1.4952\n",
            "Epoch: 2/15, Batch: 500, Loss: 1.5868\n",
            "Epoch: 2/15, Batch: 600, Loss: 1.7410\n",
            "Epoch: 2/15, Batch: 700, Loss: 1.4865\n",
            "Epoch 2/15:\n",
            "  Train Loss: 1.5889, Train Acc: 38.89%\n",
            "  Test Acc: 42.86%\n",
            "--------------------------------------------------\n",
            "Epoch: 3/15, Batch: 0, Loss: 1.5473\n",
            "Epoch: 3/15, Batch: 100, Loss: 1.3800\n",
            "Epoch: 3/15, Batch: 200, Loss: 1.5131\n",
            "Epoch: 3/15, Batch: 300, Loss: 1.3705\n",
            "Epoch: 3/15, Batch: 400, Loss: 1.5224\n",
            "Epoch: 3/15, Batch: 500, Loss: 1.6452\n",
            "Epoch: 3/15, Batch: 600, Loss: 1.2997\n",
            "Epoch: 3/15, Batch: 700, Loss: 1.1932\n",
            "Epoch 3/15:\n",
            "  Train Loss: 1.3389, Train Acc: 49.78%\n",
            "  Test Acc: 53.05%\n",
            "--------------------------------------------------\n",
            "Epoch: 4/15, Batch: 0, Loss: 1.1236\n",
            "Epoch: 4/15, Batch: 100, Loss: 1.2699\n",
            "Epoch: 4/15, Batch: 200, Loss: 1.0089\n",
            "Epoch: 4/15, Batch: 300, Loss: 0.9435\n",
            "Epoch: 4/15, Batch: 400, Loss: 1.4072\n",
            "Epoch: 4/15, Batch: 500, Loss: 1.1051\n",
            "Epoch: 4/15, Batch: 600, Loss: 1.1675\n",
            "Epoch: 4/15, Batch: 700, Loss: 0.9496\n",
            "Epoch 4/15:\n",
            "  Train Loss: 1.1618, Train Acc: 58.06%\n",
            "  Test Acc: 62.54%\n",
            "--------------------------------------------------\n",
            "Epoch: 5/15, Batch: 0, Loss: 1.1202\n",
            "Epoch: 5/15, Batch: 100, Loss: 1.1362\n",
            "Epoch: 5/15, Batch: 200, Loss: 0.8268\n",
            "Epoch: 5/15, Batch: 300, Loss: 1.1690\n",
            "Epoch: 5/15, Batch: 400, Loss: 1.1251\n",
            "Epoch: 5/15, Batch: 500, Loss: 1.0302\n",
            "Epoch: 5/15, Batch: 600, Loss: 0.9507\n",
            "Epoch: 5/15, Batch: 700, Loss: 0.9720\n",
            "Epoch 5/15:\n",
            "  Train Loss: 1.0472, Train Acc: 62.37%\n",
            "  Test Acc: 58.71%\n",
            "--------------------------------------------------\n",
            "Epoch: 6/15, Batch: 0, Loss: 0.8099\n",
            "Epoch: 6/15, Batch: 100, Loss: 0.9506\n",
            "Epoch: 6/15, Batch: 200, Loss: 1.1032\n",
            "Epoch: 6/15, Batch: 300, Loss: 1.0848\n",
            "Epoch: 6/15, Batch: 400, Loss: 0.8643\n",
            "Epoch: 6/15, Batch: 500, Loss: 0.8595\n",
            "Epoch: 6/15, Batch: 600, Loss: 0.8899\n",
            "Epoch: 6/15, Batch: 700, Loss: 0.8432\n",
            "Epoch 6/15:\n",
            "  Train Loss: 0.9710, Train Acc: 65.45%\n",
            "  Test Acc: 65.89%\n",
            "--------------------------------------------------\n",
            "Epoch: 7/15, Batch: 0, Loss: 1.0613\n",
            "Epoch: 7/15, Batch: 100, Loss: 1.0916\n",
            "Epoch: 7/15, Batch: 200, Loss: 0.8936\n",
            "Epoch: 7/15, Batch: 300, Loss: 1.0054\n",
            "Epoch: 7/15, Batch: 400, Loss: 0.8745\n",
            "Epoch: 7/15, Batch: 500, Loss: 0.9515\n",
            "Epoch: 7/15, Batch: 600, Loss: 0.8869\n",
            "Epoch: 7/15, Batch: 700, Loss: 1.1237\n",
            "Epoch 7/15:\n",
            "  Train Loss: 0.8942, Train Acc: 68.26%\n",
            "  Test Acc: 67.48%\n",
            "--------------------------------------------------\n",
            "Epoch: 8/15, Batch: 0, Loss: 0.9256\n",
            "Epoch: 8/15, Batch: 100, Loss: 0.7029\n",
            "Epoch: 8/15, Batch: 200, Loss: 0.8018\n",
            "Epoch: 8/15, Batch: 300, Loss: 0.7792\n",
            "Epoch: 8/15, Batch: 400, Loss: 0.8231\n",
            "Epoch: 8/15, Batch: 500, Loss: 0.7721\n",
            "Epoch: 8/15, Batch: 600, Loss: 0.7891\n",
            "Epoch: 8/15, Batch: 700, Loss: 0.9404\n",
            "Epoch 8/15:\n",
            "  Train Loss: 0.7210, Train Acc: 74.54%\n",
            "  Test Acc: 75.93%\n",
            "--------------------------------------------------\n",
            "Epoch: 9/15, Batch: 0, Loss: 0.6946\n",
            "Epoch: 9/15, Batch: 100, Loss: 0.8059\n",
            "Epoch: 9/15, Batch: 200, Loss: 0.6399\n",
            "Epoch: 9/15, Batch: 300, Loss: 0.5717\n",
            "Epoch: 9/15, Batch: 400, Loss: 0.6836\n",
            "Epoch: 9/15, Batch: 500, Loss: 0.5789\n",
            "Epoch: 9/15, Batch: 600, Loss: 0.8365\n",
            "Epoch: 9/15, Batch: 700, Loss: 0.4898\n",
            "Epoch 9/15:\n",
            "  Train Loss: 0.6733, Train Acc: 76.19%\n",
            "  Test Acc: 76.67%\n",
            "--------------------------------------------------\n",
            "Epoch: 10/15, Batch: 0, Loss: 0.6755\n",
            "Epoch: 10/15, Batch: 100, Loss: 0.7316\n",
            "Epoch: 10/15, Batch: 200, Loss: 0.8225\n",
            "Epoch: 10/15, Batch: 300, Loss: 0.6607\n",
            "Epoch: 10/15, Batch: 400, Loss: 0.6166\n",
            "Epoch: 10/15, Batch: 500, Loss: 0.5002\n",
            "Epoch: 10/15, Batch: 600, Loss: 0.4626\n",
            "Epoch: 10/15, Batch: 700, Loss: 0.7187\n",
            "Epoch 10/15:\n",
            "  Train Loss: 0.6461, Train Acc: 77.28%\n",
            "  Test Acc: 77.16%\n",
            "--------------------------------------------------\n",
            "Epoch: 11/15, Batch: 0, Loss: 0.6722\n",
            "Epoch: 11/15, Batch: 100, Loss: 0.4696\n",
            "Epoch: 11/15, Batch: 200, Loss: 0.4868\n",
            "Epoch: 11/15, Batch: 300, Loss: 0.6689\n",
            "Epoch: 11/15, Batch: 400, Loss: 0.4641\n",
            "Epoch: 11/15, Batch: 500, Loss: 0.6398\n",
            "Epoch: 11/15, Batch: 600, Loss: 0.7306\n",
            "Epoch: 11/15, Batch: 700, Loss: 0.6256\n",
            "Epoch 11/15:\n",
            "  Train Loss: 0.6270, Train Acc: 78.02%\n",
            "  Test Acc: 77.71%\n",
            "--------------------------------------------------\n",
            "Epoch: 12/15, Batch: 0, Loss: 0.5565\n",
            "Epoch: 12/15, Batch: 100, Loss: 0.5131\n",
            "Epoch: 12/15, Batch: 200, Loss: 0.4587\n",
            "Epoch: 12/15, Batch: 300, Loss: 0.5283\n",
            "Epoch: 12/15, Batch: 400, Loss: 0.3959\n",
            "Epoch: 12/15, Batch: 500, Loss: 0.5227\n",
            "Epoch: 12/15, Batch: 600, Loss: 0.6580\n",
            "Epoch: 12/15, Batch: 700, Loss: 0.7522\n",
            "Epoch 12/15:\n",
            "  Train Loss: 0.6088, Train Acc: 78.77%\n",
            "  Test Acc: 78.07%\n",
            "--------------------------------------------------\n",
            "Epoch: 13/15, Batch: 0, Loss: 0.5390\n",
            "Epoch: 13/15, Batch: 100, Loss: 0.4285\n",
            "Epoch: 13/15, Batch: 200, Loss: 0.6026\n",
            "Epoch: 13/15, Batch: 300, Loss: 0.4684\n",
            "Epoch: 13/15, Batch: 400, Loss: 0.4378\n",
            "Epoch: 13/15, Batch: 500, Loss: 0.5694\n",
            "Epoch: 13/15, Batch: 600, Loss: 0.5754\n",
            "Epoch: 13/15, Batch: 700, Loss: 0.6357\n",
            "Epoch 13/15:\n",
            "  Train Loss: 0.5932, Train Acc: 79.23%\n",
            "  Test Acc: 78.61%\n",
            "--------------------------------------------------\n",
            "Epoch: 14/15, Batch: 0, Loss: 0.7061\n",
            "Epoch: 14/15, Batch: 100, Loss: 0.4019\n",
            "Epoch: 14/15, Batch: 200, Loss: 0.7781\n",
            "Epoch: 14/15, Batch: 300, Loss: 0.4290\n",
            "Epoch: 14/15, Batch: 400, Loss: 0.3942\n",
            "Epoch: 14/15, Batch: 500, Loss: 0.4997\n",
            "Epoch: 14/15, Batch: 600, Loss: 0.5353\n",
            "Epoch: 14/15, Batch: 700, Loss: 0.6324\n",
            "Epoch 14/15:\n",
            "  Train Loss: 0.5778, Train Acc: 79.91%\n",
            "  Test Acc: 79.31%\n",
            "--------------------------------------------------\n",
            "Epoch: 15/15, Batch: 0, Loss: 0.5626\n",
            "Epoch: 15/15, Batch: 100, Loss: 0.7163\n",
            "Epoch: 15/15, Batch: 200, Loss: 0.5049\n",
            "Epoch: 15/15, Batch: 300, Loss: 0.5231\n",
            "Epoch: 15/15, Batch: 400, Loss: 0.7026\n",
            "Epoch: 15/15, Batch: 500, Loss: 0.6193\n",
            "Epoch: 15/15, Batch: 600, Loss: 0.7062\n",
            "Epoch: 15/15, Batch: 700, Loss: 0.7279\n",
            "Epoch 15/15:\n",
            "  Train Loss: 0.5461, Train Acc: 80.97%\n",
            "  Test Acc: 80.02%\n",
            "--------------------------------------------------\n",
            "\n",
            "6. Final Results:\n",
            "Best training accuracy: 80.97%\n",
            "Best test accuracy: 80.02%\n",
            "Final test accuracy: 80.02%\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    }
  ]
}