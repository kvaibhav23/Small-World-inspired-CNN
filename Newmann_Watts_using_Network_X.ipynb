{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZazxpEmjiOGP",
        "outputId": "4a98b7f3-3191-46a2-84ed-f9dd8d22f328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNBCN: Convolutional Neural Network Based on Complex Networks\n",
            "Implementation with Newman-Watts Small-World Networks (36 Nodes)\n",
            "======================================================================\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 14.7 GB\n",
            "\n",
            "============================================================\n",
            "Graph Algorithm Analysis\n",
            "============================================================\n",
            "Newman-Watts Small-World Network Analysis (NetworkX)\n",
            "==================================================\n",
            "\n",
            "Newman-Watts (k=4, p=0.1):\n",
            "  Initial ring lattice edges: 72\n",
            "  Random edges added: 8\n",
            "  Total edges: 80\n",
            "  Average degree: 4.44\n",
            "  Clustering coefficient: 0.4435\n",
            "  Average shortest path: 2.9698\n",
            "\n",
            "============================================================\n",
            "Model Analysis\n",
            "============================================================\n",
            "\n",
            "DAG Analysis:\n",
            "==================================================\n",
            "\n",
            "Module 1 DAG (Newman-Watts):\n",
            "  Original edges: 72\n",
            "  Random edges: 8\n",
            "  Total nodes: 36\n",
            "  Total edges: 80\n",
            "  Max in-degree: 4\n",
            "  Max out-degree: 5\n",
            "  Avg in-degree: 2.22\n",
            "  Avg out-degree: 2.22\n",
            "  Input nodes: 1 - [0]\n",
            "  Output nodes: 1 - [35]\n",
            "\n",
            "Module 2 DAG (Newman-Watts):\n",
            "  Original edges: 72\n",
            "  Random edges: 10\n",
            "  Total nodes: 36\n",
            "  Total edges: 82\n",
            "  Max in-degree: 5\n",
            "  Max out-degree: 5\n",
            "  Avg in-degree: 2.28\n",
            "  Avg out-degree: 2.28\n",
            "  Input nodes: 1 - [0]\n",
            "  Output nodes: 1 - [35]\n",
            "\n",
            "Module 3 DAG (Newman-Watts):\n",
            "  Original edges: 72\n",
            "  Random edges: 5\n",
            "  Total nodes: 36\n",
            "  Total edges: 77\n",
            "  Max in-degree: 4\n",
            "  Max out-degree: 5\n",
            "  Avg in-degree: 2.14\n",
            "  Avg out-degree: 2.14\n",
            "  Input nodes: 1 - [0]\n",
            "  Output nodes: 1 - [35]\n",
            "\n",
            "Module 4 DAG (Newman-Watts):\n",
            "  Original edges: 72\n",
            "  Random edges: 15\n",
            "  Total nodes: 36\n",
            "  Total edges: 87\n",
            "  Max in-degree: 4\n",
            "  Max out-degree: 6\n",
            "  Avg in-degree: 2.42\n",
            "  Avg out-degree: 2.42\n",
            "  Input nodes: 1 - [0]\n",
            "  Output nodes: 1 - [35]\n",
            "\n",
            "============================================================\n",
            "Training Newman-Watts CNNBCN Model (36 Nodes)\n",
            "============================================================\n",
            "Training on device: cuda\n",
            "Model parameters: 28,734,426\n",
            "Epoch: 1/15, Batch: 0/782, Loss: 2.4651\n",
            "Epoch: 1/15, Batch: 200/782, Loss: 2.1679\n",
            "Epoch: 1/15, Batch: 400/782, Loss: 1.7832\n",
            "Epoch: 1/15, Batch: 600/782, Loss: 1.6769\n",
            "Epoch 1/15 (275.3s):\n",
            "  Train Loss: 1.9037, Train Acc: 25.75%\n",
            "  Test Loss: 1.6371, Test Acc: 37.40%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 2/15, Batch: 0/782, Loss: 1.7953\n",
            "Epoch: 2/15, Batch: 200/782, Loss: 1.6771\n",
            "Epoch: 2/15, Batch: 400/782, Loss: 1.5038\n",
            "Epoch: 2/15, Batch: 600/782, Loss: 1.4458\n",
            "Epoch 2/15 (273.0s):\n",
            "  Train Loss: 1.4578, Train Acc: 45.92%\n",
            "  Test Loss: 1.4594, Test Acc: 48.50%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 3/15, Batch: 0/782, Loss: 1.3709\n",
            "Epoch: 3/15, Batch: 200/782, Loss: 1.3868\n",
            "Epoch: 3/15, Batch: 400/782, Loss: 1.1930\n",
            "Epoch: 3/15, Batch: 600/782, Loss: 1.1183\n",
            "Epoch 3/15 (271.0s):\n",
            "  Train Loss: 1.2027, Train Acc: 56.65%\n",
            "  Test Loss: 1.1662, Test Acc: 57.88%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 4/15, Batch: 0/782, Loss: 1.1538\n",
            "Epoch: 4/15, Batch: 200/782, Loss: 1.1404\n",
            "Epoch: 4/15, Batch: 400/782, Loss: 1.2587\n",
            "Epoch: 4/15, Batch: 600/782, Loss: 1.1294\n",
            "Epoch 4/15 (270.6s):\n",
            "  Train Loss: 1.0596, Train Acc: 62.23%\n",
            "  Test Loss: 1.0707, Test Acc: 61.80%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 5/15, Batch: 0/782, Loss: 0.8856\n",
            "Epoch: 5/15, Batch: 200/782, Loss: 1.0031\n",
            "Epoch: 5/15, Batch: 400/782, Loss: 0.9666\n",
            "Epoch: 5/15, Batch: 600/782, Loss: 1.1023\n",
            "Epoch 5/15 (268.6s):\n",
            "  Train Loss: 0.9580, Train Acc: 66.06%\n",
            "  Test Loss: 0.9117, Test Acc: 68.83%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 6/15, Batch: 0/782, Loss: 0.8365\n",
            "Epoch: 6/15, Batch: 200/782, Loss: 0.7905\n",
            "Epoch: 6/15, Batch: 400/782, Loss: 0.9504\n",
            "Epoch: 6/15, Batch: 600/782, Loss: 0.9798\n",
            "Epoch 6/15 (268.4s):\n",
            "  Train Loss: 0.8728, Train Acc: 69.74%\n",
            "  Test Loss: 0.8312, Test Acc: 70.75%\n",
            "  Learning Rate: 0.001000\n",
            "------------------------------------------------------------\n",
            "Epoch: 7/15, Batch: 0/782, Loss: 0.7765\n",
            "Epoch: 7/15, Batch: 200/782, Loss: 1.0284\n",
            "Epoch: 7/15, Batch: 400/782, Loss: 0.6781\n",
            "Epoch: 7/15, Batch: 600/782, Loss: 0.9484\n",
            "Epoch 7/15 (268.3s):\n",
            "  Train Loss: 0.8033, Train Acc: 72.38%\n",
            "  Test Loss: 0.8282, Test Acc: 72.69%\n",
            "  Learning Rate: 0.000100\n",
            "------------------------------------------------------------\n",
            "Epoch: 8/15, Batch: 0/782, Loss: 0.7590\n",
            "Epoch: 8/15, Batch: 200/782, Loss: 0.5045\n",
            "Epoch: 8/15, Batch: 400/782, Loss: 0.5954\n",
            "Epoch: 8/15, Batch: 600/782, Loss: 0.6834\n",
            "Epoch 8/15 (267.4s):\n",
            "  Train Loss: 0.6324, Train Acc: 78.33%\n",
            "  Test Loss: 0.5887, Test Acc: 79.77%\n",
            "  Learning Rate: 0.000100\n",
            "------------------------------------------------------------\n",
            "Epoch: 9/15, Batch: 0/782, Loss: 0.5515\n",
            "Epoch: 9/15, Batch: 200/782, Loss: 0.4174\n",
            "Epoch: 9/15, Batch: 400/782, Loss: 0.5616\n",
            "Epoch: 9/15, Batch: 600/782, Loss: 0.6571\n",
            "Epoch 9/15 (266.0s):\n",
            "  Train Loss: 0.5926, Train Acc: 79.68%\n",
            "  Test Loss: 0.5668, Test Acc: 80.44%\n",
            "  Learning Rate: 0.000100\n",
            "------------------------------------------------------------\n",
            "Epoch: 10/15, Batch: 0/782, Loss: 0.6911\n",
            "Epoch: 10/15, Batch: 200/782, Loss: 0.5562\n",
            "Epoch: 10/15, Batch: 400/782, Loss: 0.5030\n",
            "Epoch: 10/15, Batch: 600/782, Loss: 0.5903\n",
            "Epoch 10/15 (265.4s):\n",
            "  Train Loss: 0.5637, Train Acc: 80.71%\n",
            "  Test Loss: 0.5499, Test Acc: 80.93%\n",
            "  Learning Rate: 0.000100\n",
            "------------------------------------------------------------\n",
            "Epoch: 11/15, Batch: 0/782, Loss: 0.5310\n",
            "Epoch: 11/15, Batch: 200/782, Loss: 0.6506\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class DAGConverter:\n",
        "    \"\"\"Convert undirected graph to Directed Acyclic Graph\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def convert_to_dag(self, graph: nx.Graph) -> nx.DiGraph:\n",
        "        \"\"\"\n",
        "        Convert undirected graph to DAG by assigning indices and directing edges\n",
        "        from smaller to larger index (as described in paper)\n",
        "        \"\"\"\n",
        "        # Create directed graph\n",
        "        dag = nx.DiGraph()\n",
        "\n",
        "        # Add all nodes\n",
        "        dag.add_nodes_from(graph.nodes())\n",
        "\n",
        "        # Add directed edges from smaller to larger index\n",
        "        for u, v in graph.edges():\n",
        "            if u < v:\n",
        "                dag.add_edge(u, v)\n",
        "            else:\n",
        "                dag.add_edge(v, u)\n",
        "\n",
        "        return dag\n",
        "\n",
        "class NodeOperation(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network node operation as described in paper\n",
        "    Performs: Aggregation -> Transformation -> Distribution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, max_inputs: int = 15, use_gelu: bool = False):\n",
        "        super(NodeOperation, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.max_inputs = max_inputs\n",
        "\n",
        "        # Aggregation weights (learnable and positive via sigmoid)\n",
        "        self.aggregation_weights = nn.Parameter(torch.randn(max_inputs))\n",
        "\n",
        "        # Transformation operations\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Activation function (GeLU for first modules, ReLU for others as per paper)\n",
        "        if use_gelu:\n",
        "            self.activation = nn.GELU()\n",
        "        else:\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass implementing the three steps from paper:\n",
        "        1. Aggregation: weighted sum of inputs\n",
        "        2. Transformation: conv + batch_norm + activation\n",
        "        3. Distribution: output to next nodes\n",
        "        \"\"\"\n",
        "        if not inputs:\n",
        "            raise ValueError(\"Node must have at least one input\")\n",
        "\n",
        "        # Step 1: Aggregation with learnable weights\n",
        "        if len(inputs) == 1:\n",
        "            aggregated = inputs[0]\n",
        "        else:\n",
        "            # Limit the number of inputs to max_inputs to prevent index errors\n",
        "            num_inputs = min(len(inputs), self.max_inputs)\n",
        "            limited_inputs = inputs[:num_inputs]\n",
        "\n",
        "            # Apply sigmoid to ensure positive weights\n",
        "            weights = torch.sigmoid(self.aggregation_weights[:num_inputs])\n",
        "            weights = weights / weights.sum()  # Normalize weights\n",
        "\n",
        "            # Weighted sum of inputs\n",
        "            aggregated = torch.zeros_like(limited_inputs[0])\n",
        "            for i, inp in enumerate(limited_inputs):\n",
        "                aggregated += weights[i] * inp\n",
        "\n",
        "        # Step 2: Transformation\n",
        "        x = self.conv(aggregated)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CNNBCNModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Single CNNBCN module generated from DAG\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dag: nx.DiGraph, in_channels: int, out_channels: int, use_gelu: bool = False):\n",
        "        super(CNNBCNModule, self).__init__()\n",
        "        self.dag = dag\n",
        "        self.nodes = list(dag.nodes())\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # Calculate max inputs for any node to size the aggregation weights properly\n",
        "        max_inputs = max([dag.in_degree(node) for node in self.nodes] + [1])\n",
        "        max_inputs = max(max_inputs, 15)  # Ensure minimum capacity\n",
        "\n",
        "        # Create node operations\n",
        "        self.node_ops = nn.ModuleDict()\n",
        "        for node in self.nodes:\n",
        "            self.node_ops[str(node)] = NodeOperation(in_channels, out_channels, max_inputs, use_gelu)\n",
        "\n",
        "        # Input and output node mappings\n",
        "        self.input_nodes = [node for node in self.nodes if dag.in_degree(node) == 0]\n",
        "        self.output_nodes = [node for node in self.nodes if dag.out_degree(node) == 0]\n",
        "\n",
        "        # If no natural input/output nodes, use first and last\n",
        "        if not self.input_nodes:\n",
        "            self.input_nodes = [min(self.nodes)]\n",
        "        if not self.output_nodes:\n",
        "            self.output_nodes = [max(self.nodes)]\n",
        "\n",
        "        # Input projection to distribute input to input nodes\n",
        "        self.input_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        # Output aggregation\n",
        "        if len(self.output_nodes) > 1:\n",
        "            self.output_aggregation = nn.Conv2d(out_channels * len(self.output_nodes), out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.output_aggregation = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Project input\n",
        "        x_proj = self.input_projection(x)\n",
        "\n",
        "        # Initialize node outputs\n",
        "        node_outputs = {}\n",
        "\n",
        "        # Topological sort for processing order\n",
        "        topo_order = list(nx.topological_sort(self.dag))\n",
        "\n",
        "        # Process nodes in topological order\n",
        "        for node in topo_order:\n",
        "            if node in self.input_nodes:\n",
        "                # Input nodes receive the projected input\n",
        "                node_inputs = [x_proj]\n",
        "            else:\n",
        "                # Collect inputs from predecessor nodes\n",
        "                predecessors = list(self.dag.predecessors(node))\n",
        "                node_inputs = [node_outputs[pred] for pred in predecessors if pred in node_outputs]\n",
        "\n",
        "            if node_inputs:\n",
        "                node_outputs[node] = self.node_ops[str(node)](node_inputs)\n",
        "\n",
        "        # Aggregate outputs from output nodes\n",
        "        output_tensors = [node_outputs[node] for node in self.output_nodes if node in node_outputs]\n",
        "\n",
        "        if not output_tensors:\n",
        "            # Fallback: use last computed node output\n",
        "            output_tensors = [list(node_outputs.values())[-1]]\n",
        "\n",
        "        if len(output_tensors) == 1:\n",
        "            return output_tensors[0]\n",
        "        else:\n",
        "            # Concatenate and aggregate multiple outputs\n",
        "            concatenated = torch.cat(output_tensors, dim=1)\n",
        "            return self.output_aggregation(concatenated)\n",
        "\n",
        "class CNNBCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete CNNBCN model with Newman-Watts small-world modules\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, num_modules: int = 4, nodes_per_module: int = 36):\n",
        "        super(CNNBCN, self).__init__()\n",
        "\n",
        "        self.num_modules = num_modules\n",
        "        self.nodes_per_module = nodes_per_module\n",
        "\n",
        "        # Generate random graphs and convert to DAGs for each module\n",
        "        self.dags = []\n",
        "        self.graph_stats = []\n",
        "\n",
        "        for i in range(num_modules):\n",
        "            # Generate Newman-Watts small-world graph using NetworkX\n",
        "            graph = nx.newman_watts_strogatz_graph(\n",
        "                n=nodes_per_module,\n",
        "                k=4,\n",
        "                p=0.1,\n",
        "                seed=42 + i  # Different seed for each module\n",
        "            )\n",
        "\n",
        "            # Calculate graph statistics\n",
        "            initial_edges = (nodes_per_module * 4) // 2  # Initial ring lattice edges\n",
        "            random_edges = graph.number_of_edges() - initial_edges\n",
        "            stats = {\n",
        "                'algorithm': 'Newman-Watts',\n",
        "                'original_edges': initial_edges,\n",
        "                'random_edges': random_edges,\n",
        "                'total_edges': graph.number_of_edges(),\n",
        "                'nodes': nodes_per_module\n",
        "            }\n",
        "\n",
        "            # Convert to DAG\n",
        "            dag_converter = DAGConverter()\n",
        "            dag = dag_converter.convert_to_dag(graph)\n",
        "            self.dags.append(dag)\n",
        "            self.graph_stats.append(stats)\n",
        "\n",
        "        # Channel progression\n",
        "        channels = [32, 64, 128, 256]\n",
        "\n",
        "        # Input layer\n",
        "        self.input_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1)\n",
        "        self.input_bn = nn.BatchNorm2d(channels[0])\n",
        "        self.input_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # CNNBCN modules\n",
        "        self.modules_list = nn.ModuleList()\n",
        "        for i in range(num_modules):\n",
        "            in_ch = channels[i] if i < len(channels) else channels[-1]\n",
        "            out_ch = channels[i] if i < len(channels) else channels[-1]\n",
        "\n",
        "            # Use GeLU for first two modules, ReLU for others\n",
        "            use_gelu = i < 2\n",
        "\n",
        "            module = CNNBCNModule(self.dags[i], in_ch, out_ch, use_gelu)\n",
        "            self.modules_list.append(module)\n",
        "\n",
        "        # Downsampling layers between modules\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        for i in range(num_modules - 1):\n",
        "            if i < len(channels) - 1:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(channels[i], channels[i+1], kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(channels[i+1]),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(channels[-1], channels[-1], kernel_size=3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(channels[-1]),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            self.downsample_layers.append(downsample)\n",
        "\n",
        "        # Global average pooling and classifier\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(channels[-1], num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Input processing\n",
        "        x = self.input_conv(x)\n",
        "        x = self.input_bn(x)\n",
        "        x = self.input_relu(x)\n",
        "\n",
        "        # Pass through CNNBCN modules\n",
        "        for i, module in enumerate(self.modules_list):\n",
        "            x = module(x)\n",
        "\n",
        "            # Downsample between modules (except last)\n",
        "            if i < len(self.downsample_layers):\n",
        "                x = self.downsample_layers[i](x)\n",
        "\n",
        "        # Global average pooling and classification\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def get_cifar10_dataloaders(batch_size: int = 64, num_workers: int = 2):\n",
        "    \"\"\"Get CIFAR-10 data loaders\"\"\"\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=train_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs=10, lr=0.001):\n",
        "    \"\"\"Train the CNNBCN model\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total_train += target.size(0)\n",
        "            correct_train += predicted.eq(target).sum().item()\n",
        "\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate training metrics\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct_train / total_train\n",
        "\n",
        "        # Test phase\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item()\n",
        "                _, predicted = output.max(1)\n",
        "                total_test += target.size(0)\n",
        "                correct_test += predicted.eq(target).sum().item()\n",
        "\n",
        "        test_acc = 100. * correct_test / total_test\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s):')\n",
        "        print(f'  Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "        print('-' * 60)\n",
        "\n",
        "    return train_losses, train_accuracies, test_accuracies\n",
        "\n",
        "def analyze_newman_watts_properties(n_nodes=36, k=4, p=0.1):\n",
        "    \"\"\"Analyze Newman-Watts small-world network properties using NetworkX\"\"\"\n",
        "\n",
        "    print(\"Newman-Watts Small-World Network Analysis (NetworkX)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate Newman-Watts graph\n",
        "    graph = nx.newman_watts_strogatz_graph(n=n_nodes, k=k, p=p, seed=42)\n",
        "\n",
        "    # Calculate statistics\n",
        "    initial_edges = (n_nodes * k) // 2  # Initial ring lattice edges\n",
        "    random_edges = graph.number_of_edges() - initial_edges\n",
        "\n",
        "    print(f\"\\nNewman-Watts (k={k}, p={p}):\")\n",
        "    print(f\"  Initial ring lattice edges: {initial_edges}\")\n",
        "    print(f\"  Random edges added: {random_edges}\")\n",
        "    print(f\"  Total edges: {graph.number_of_edges()}\")\n",
        "    print(f\"  Average degree: {2 * graph.number_of_edges() / n_nodes:.2f}\")\n",
        "    print(f\"  Clustering coefficient: {nx.average_clustering(graph):.4f}\")\n",
        "    print(f\"  Average shortest path: {nx.average_shortest_path_length(graph):.4f}\")\n",
        "\n",
        "    return graph\n",
        "\n",
        "def analyze_dag_properties(dags, graph_stats):\n",
        "    \"\"\"Analyze properties of generated DAGs\"\"\"\n",
        "    print(\"\\nDAG Analysis:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, (dag, stats) in enumerate(zip(dags, graph_stats)):\n",
        "        print(f\"\\nModule {i+1} DAG (Newman-Watts):\")\n",
        "        print(f\"  Original edges: {stats.get('original_edges', 'N/A')}\")\n",
        "        print(f\"  Random edges: {stats.get('random_edges', 'N/A')}\")\n",
        "        print(f\"  Total nodes: {dag.number_of_nodes()}\")\n",
        "        print(f\"  Total edges: {dag.number_of_edges()}\")\n",
        "\n",
        "        # Calculate in-degree and out-degree statistics\n",
        "        in_degrees = [dag.in_degree(node) for node in dag.nodes()]\n",
        "        out_degrees = [dag.out_degree(node) for node in dag.nodes()]\n",
        "\n",
        "        print(f\"  Max in-degree: {max(in_degrees)}\")\n",
        "        print(f\"  Max out-degree: {max(out_degrees)}\")\n",
        "        print(f\"  Avg in-degree: {np.mean(in_degrees):.2f}\")\n",
        "        print(f\"  Avg out-degree: {np.mean(out_degrees):.2f}\")\n",
        "\n",
        "        # Count input and output nodes\n",
        "        input_nodes = [node for node in dag.nodes() if dag.in_degree(node) == 0]\n",
        "        output_nodes = [node for node in dag.nodes() if dag.out_degree(node) == 0]\n",
        "\n",
        "        print(f\"  Input nodes: {len(input_nodes)} - {input_nodes[:5]}{'...' if len(input_nodes) > 5 else ''}\")\n",
        "        print(f\"  Output nodes: {len(output_nodes)} - {output_nodes[:5]}{'...' if len(output_nodes) > 5 else ''}\")\n",
        "\n",
        "def visualize_training_results(train_losses, train_accs, test_accs):\n",
        "    \"\"\"Visualize training results with plots\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
        "    ax1.set_title('Training Loss Over Epochs')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
        "    ax2.plot(test_accs, label='Test Accuracy', color='red')\n",
        "    ax2.set_title('Accuracy Over Epochs')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.suptitle('Newman-Watts CNNBCN Training Results (36 Nodes)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_model_checkpoint(model, optimizer, epoch, loss, accuracy, filename):\n",
        "    \"\"\"Save model checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "        'graph_stats': model.graph_stats if hasattr(model, 'graph_stats') else None\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"Checkpoint saved: {filename}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run Newman-Watts CNNBCN experiment\"\"\"\n",
        "    print(\"CNNBCN: Convolutional Neural Network Based on Complex Networks\")\n",
        "    print(\"Implementation with Newman-Watts Small-World Networks (36 Nodes)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set device info\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "    # Analyze Newman-Watts network properties\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Graph Algorithm Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    nw_graph = analyze_newman_watts_properties(n_nodes=36, k=4, p=0.1)\n",
        "\n",
        "    # Load data\n",
        "    train_loader, test_loader = get_cifar10_dataloaders(batch_size=64)\n",
        "\n",
        "    # Create model with 36 nodes per module\n",
        "    model = CNNBCN(num_classes=10, num_modules=4, nodes_per_module=36)\n",
        "\n",
        "    # Analyze model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Model Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    analyze_dag_properties(model.dags, model.graph_stats)\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Training Newman-Watts CNNBCN Model (36 Nodes)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_losses, train_accs, test_accs = train_model(\n",
        "        model, train_loader, test_loader, num_epochs=15, lr=0.001\n",
        "    )\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_training_results(train_losses, train_accs, test_accs)\n",
        "\n",
        "    # Save model checkpoint\n",
        "    save_model_checkpoint(model, None, 15, train_losses[-1], test_accs[-1],\n",
        "                         \"cnnbcn_newman_watts_36nodes_final.pth\")\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Final Results\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Best Test Accuracy: {max(test_accs):.2f}%\")\n",
        "    print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")\n",
        "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    return model, train_losses, train_accs, test_accs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}