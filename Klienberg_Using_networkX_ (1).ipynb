{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZazxpEmjiOGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf83835c-1a5e-47de-ec08-a4c35b732939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNBCN with NetworkX Kleinberg Small-World Networks (36 nodes) for CIFAR-10\n",
            "================================================================================\n",
            "Loading CIFAR-10 dataset...\n",
            "Creating CNNBCN model with NetworkX Kleinberg networks (36 nodes each)...\n",
            "Total parameters: 9,850,087\n",
            "Trainable parameters: 9,850,087\n",
            "Nodes per module: 36\n",
            "Starting training...\n",
            "Epoch 1/15, Batch 0, Loss: 2.3464\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class KleinbergGraphGenerator:\n",
        "    \"\"\"\n",
        "    Kleinberg small-world network generator using NetworkX implementation\n",
        "    Creates a 6x6 grid (36 nodes) with long-range connections\n",
        "    \"\"\"\n",
        "    def __init__(self, n=6, p=2, q=1, r=2, dim=2, seed=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n: Linear dimension of the lattice (6 for 36 nodes in 2D)\n",
        "            p: Exponent for computing connection probabilities\n",
        "            q: Number of local contacts per node\n",
        "            r: Number of long-range contacts per node\n",
        "            dim: Dimension of the underlying lattice (2 for 2D)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.r = r\n",
        "        self.dim = dim\n",
        "        self.seed = seed\n",
        "        self.total_nodes = n ** dim  # n^dim nodes for dim-dimensional lattice\n",
        "\n",
        "        # Ensure we have exactly 36 nodes\n",
        "        assert self.total_nodes == 36, f\"Expected 36 nodes, got {self.total_nodes}\"\n",
        "\n",
        "    def generate_graph(self):\n",
        "        \"\"\"\n",
        "        Generate Kleinberg small-world network using NetworkX\n",
        "        \"\"\"\n",
        "        # Use NetworkX's navigable_small_world_graph function (Kleinberg model)\n",
        "        G = nx.navigable_small_world_graph(\n",
        "            n=self.n,\n",
        "            p=self.p,\n",
        "            q=self.q,\n",
        "            r=self.r,\n",
        "            dim=self.dim,\n",
        "            seed=self.seed\n",
        "        )\n",
        "\n",
        "        # Verify we have exactly 36 nodes\n",
        "        assert G.number_of_nodes() == 36, f\"Graph has {G.number_of_nodes()} nodes, expected 36\"\n",
        "\n",
        "        return G\n",
        "\n",
        "    def convert_to_dag(self, G):\n",
        "        \"\"\"\n",
        "        Convert undirected graph to Directed Acyclic Graph (DAG)\n",
        "        using grid-based ordering\n",
        "        \"\"\"\n",
        "        # Create ordering based on lattice position\n",
        "        node_indices = {}\n",
        "        for i, node in enumerate(G.nodes()):\n",
        "            # NetworkX navigable_small_world_graph nodes are tuples representing lattice coordinates\n",
        "            if isinstance(node, tuple):\n",
        "                # For 2D lattice, convert (i, j) to linear index\n",
        "                if len(node) == 2:\n",
        "                    i_coord, j_coord = node\n",
        "                    order = i_coord * self.n + j_coord\n",
        "                else:\n",
        "                    # For higher dimensions, use lexicographic ordering\n",
        "                    order = sum(coord * (self.n ** idx) for idx, coord in enumerate(reversed(node)))\n",
        "                node_indices[node] = order\n",
        "            else:\n",
        "                # If nodes are integers, use them directly\n",
        "                node_indices[node] = node\n",
        "\n",
        "        # Create networkx DiGraph\n",
        "        DAG = nx.DiGraph()\n",
        "        DAG.add_nodes_from(G.nodes())\n",
        "\n",
        "        # Add edges directed from smaller to larger order\n",
        "        for u, v in G.edges():\n",
        "            if node_indices[u] < node_indices[v]:\n",
        "                DAG.add_edge(u, v)\n",
        "            elif node_indices[u] > node_indices[v]:\n",
        "                DAG.add_edge(v, u)\n",
        "            # If node_indices[u] == node_indices[v], this case should not happen\n",
        "            # with unique lattice-based ordering, but if it does, we can ignore the edge\n",
        "            # or handle based on specific requirements (e.g., remove to avoid potential cycles)\n",
        "\n",
        "\n",
        "        # Verify the graph is a DAG\n",
        "        if not nx.is_directed_acyclic_graph(DAG):\n",
        "             # If it's not a DAG, there must be a cycle.\n",
        "             # This might happen if the original graph generation or node ordering\n",
        "             # creates a situation where directing edges based purely on index\n",
        "             # still results in a cycle, though the lattice ordering should prevent this.\n",
        "             # As a fallback, we can try to break cycles. A simple way is to\n",
        "             # remove edges that are part of a cycle.\n",
        "             try:\n",
        "                 cycles = list(nx.simple_cycles(DAG))\n",
        "                 for cycle in cycles:\n",
        "                     # Remove one edge from each cycle. Removing the edge with\n",
        "                     # the largest index difference might be a heuristic,\n",
        "                     # or just remove the first edge.\n",
        "                     if cycle:\n",
        "                         u, v = cycle[0], cycle[1] if len(cycle) > 1 else cycle[0] # Handle self-loops if possible\n",
        "                         if DAG.has_edge(u, v):\n",
        "                              DAG.remove_edge(u, v)\n",
        "             except nx.NetworkXNoCycle:\n",
        "                 # Should not happen based on the outer if condition, but for safety\n",
        "                 pass\n",
        "\n",
        "             # Re-check if it's a DAG after attempting to break cycles\n",
        "             if not nx.is_directed_acyclic_graph(DAG):\n",
        "                 # If still not a DAG, raise an error or handle as needed\n",
        "                 raise nx.NetworkXUnfeasible(\"Could not convert graph to a DAG after attempting to break cycles.\")\n",
        "\n",
        "\n",
        "        return DAG, node_indices\n",
        "\n",
        "class CNNBCNModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Individual CNNBCN module based on the generated Kleinberg DAG\n",
        "    \"\"\"\n",
        "    def __init__(self, dag, node_indices, num_channels, activation_type='gelu'):\n",
        "        super(CNNBCNModule, self).__init__()\n",
        "        self.dag = dag\n",
        "        self.node_indices = node_indices\n",
        "        self.num_channels = num_channels\n",
        "        self.activation_type = activation_type\n",
        "\n",
        "        # Create node operations for each node\n",
        "        self.node_ops = nn.ModuleDict()\n",
        "        self.edge_weights = nn.ParameterDict()\n",
        "\n",
        "        # Calculate in-degrees for each node\n",
        "        in_degrees = dict(dag.in_degree())\n",
        "\n",
        "        for node in dag.nodes():\n",
        "            in_degree = in_degrees[node]\n",
        "            if in_degree > 0:  # Only create operations for nodes with inputs\n",
        "                # Aggregation weights (made positive using softmax for better stability)\n",
        "                self.edge_weights[str(node)] = nn.Parameter(\n",
        "                    torch.randn(in_degree) * 0.1\n",
        "                )\n",
        "\n",
        "                # Transformation operations: Conv3x3 + BN + Activation\n",
        "                self.node_ops[str(node)] = nn.Sequential(\n",
        "                    nn.Conv2d(num_channels, num_channels, 3, padding=1),\n",
        "                    nn.BatchNorm2d(num_channels),\n",
        "                    self._get_activation(activation_type)\n",
        "                )\n",
        "\n",
        "    def _get_activation(self, activation_type):\n",
        "        \"\"\"Get activation function based on type\"\"\"\n",
        "        if activation_type == 'gelu':\n",
        "            return nn.GELU()\n",
        "        elif activation_type == 'relu':\n",
        "            return nn.ReLU(inplace=True)\n",
        "        elif activation_type == 'swish':\n",
        "            return nn.SiLU()  # Swish activation\n",
        "        else:\n",
        "            return nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the Kleinberg DAG-based module\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Store node outputs\n",
        "        node_outputs = {}\n",
        "\n",
        "        # Topological sort to process nodes in correct order\n",
        "        topo_order = list(nx.topological_sort(self.dag))\n",
        "\n",
        "        # Initialize input nodes (nodes with no predecessors)\n",
        "        input_nodes = [n for n in self.dag.nodes() if self.dag.in_degree(n) == 0]\n",
        "        for node in input_nodes:\n",
        "            node_outputs[node] = x\n",
        "\n",
        "        # Process nodes in topological order\n",
        "        for node in topo_order:\n",
        "            predecessors = list(self.dag.predecessors(node))\n",
        "\n",
        "            if len(predecessors) > 0:  # Node has inputs\n",
        "                # Aggregation: weighted sum of inputs with softmax weights\n",
        "                weights = F.softmax(self.edge_weights[str(node)], dim=0)\n",
        "\n",
        "                aggregated = None\n",
        "                for i, pred in enumerate(predecessors):\n",
        "                    if pred in node_outputs:\n",
        "                        weighted_input = weights[i] * node_outputs[pred]\n",
        "                        if aggregated is None:\n",
        "                            aggregated = weighted_input\n",
        "                        else:\n",
        "                            aggregated = aggregated + weighted_input\n",
        "\n",
        "                if aggregated is not None:\n",
        "                    # Transformation: Conv + BN + Activation\n",
        "                    transformed = self.node_ops[str(node)](aggregated)\n",
        "                    node_outputs[node] = transformed\n",
        "\n",
        "        # Collect outputs from all terminal nodes\n",
        "        output_nodes = [n for n in self.dag.nodes() if self.dag.out_degree(n) == 0]\n",
        "\n",
        "        if len(output_nodes) == 1:\n",
        "            return node_outputs.get(output_nodes[0], x)\n",
        "        else:\n",
        "            # Weighted average of outputs from multiple terminal nodes\n",
        "            outputs = [node_outputs.get(node, x) for node in output_nodes if node in node_outputs]\n",
        "            if outputs:\n",
        "                return sum(outputs) / len(outputs)\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "class CNNBCNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete CNNBCN model with Kleinberg small-world modules (36 nodes each)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, num_modules=5, kleinberg_p=2, simple_mode=True):\n",
        "        super(CNNBCNModel, self).__init__()\n",
        "\n",
        "        self.num_modules = num_modules\n",
        "        self.simple_mode = simple_mode\n",
        "        self.num_nodes = 36  # Fixed to 36 nodes per module\n",
        "\n",
        "        # Set number of channels based on mode\n",
        "        if simple_mode:\n",
        "            self.channels = 78\n",
        "        else:\n",
        "            self.channels = 109\n",
        "\n",
        "        # Initial convolution to adjust channels\n",
        "        self.input_conv = nn.Conv2d(3, self.channels, 3, padding=1)\n",
        "\n",
        "        # Generate modules using NetworkX Kleinberg algorithm\n",
        "        self.module_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_modules):\n",
        "            # Generate Kleinberg graph for this module (6x6 grid = 36 nodes)\n",
        "            # Vary parameters slightly for each module to increase diversity\n",
        "            p_param = kleinberg_p + (i * 0.2)  # Slightly increase distance decay\n",
        "\n",
        "            kleinberg_generator = KleinbergGraphGenerator(\n",
        "                n=6,  # 6^2 = 36 nodes for 2D lattice\n",
        "                p=p_param,\n",
        "                q=1,  # Number of local contacts\n",
        "                r=2,  # Number of long-range contacts\n",
        "                dim=2,\n",
        "                seed=42 + i  # Different seed for each module\n",
        "            )\n",
        "            graph = kleinberg_generator.generate_graph()\n",
        "            dag, node_indices = kleinberg_generator.convert_to_dag(graph)\n",
        "\n",
        "            # Determine activation type based on module position\n",
        "            if i < 2:  # First two modules use GeLU\n",
        "                activation_type = 'gelu'\n",
        "            elif i < 4:  # Middle modules use Swish\n",
        "                activation_type = 'swish'\n",
        "            else:  # Last modules use ReLU\n",
        "                activation_type = 'relu'\n",
        "\n",
        "            # Create module\n",
        "            module = CNNBCNModule(dag, node_indices, self.channels, activation_type)\n",
        "            self.module_list.append(module)\n",
        "\n",
        "        # Downsampling layers between modules\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        for i in range(num_modules - 1):\n",
        "            self.downsample_layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(self.channels, self.channels, 3, stride=2, padding=1),\n",
        "                    nn.BatchNorm2d(self.channels),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Final classifier with attention mechanism\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.channels, self.channels // 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.channels // 4, self.channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.channels, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial convolution\n",
        "        x = self.input_conv(x)\n",
        "\n",
        "        # Pass through modules with downsampling\n",
        "        for i, module in enumerate(self.module_list):\n",
        "            x = module(x)\n",
        "\n",
        "            # Apply downsampling except for the last module\n",
        "            if i < len(self.module_list) - 1:\n",
        "                x = self.downsample_layers[i](x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(x)\n",
        "        x = x * attention_weights\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def load_cifar10(batch_size=64):\n",
        "    \"\"\"Load CIFAR-10 dataset with appropriate transforms\"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                           download=True, transform=transform_train)\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                          download=True, transform=transform_test)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "def train_model(model, trainloader, testloader, num_epochs=15, lr=0.001):\n",
        "    \"\"\"Train the CNNBCN model with Kleinberg networks\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Added label smoothing\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_train += targets.size(0)\n",
        "            correct_train += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, '\n",
        "                      f'Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_acc = 100. * correct_train / total_train\n",
        "        train_losses.append(running_loss / len(trainloader))\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Testing phase\n",
        "        model.eval()\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in testloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += targets.size(0)\n",
        "                correct_test += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_acc = 100. * correct_test / total_test\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
        "              f'Train Loss: {train_losses[-1]:.4f}, '\n",
        "              f'Train Acc: {train_acc:.2f}%, '\n",
        "              f'Test Acc: {test_acc:.2f}%, '\n",
        "              f'Best Test Acc: {best_test_acc:.2f}%')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_losses, train_accuracies, test_accuracies\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Kleinberg CNNBCN experiment\"\"\"\n",
        "    print(\"CNNBCN with NetworkX Kleinberg Small-World Networks (36 nodes) for CIFAR-10\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "    # Load CIFAR-10 dataset\n",
        "    print(\"Loading CIFAR-10 dataset...\")\n",
        "    trainloader, testloader = load_cifar10(batch_size=64)\n",
        "\n",
        "    # Create CNNBCN model with Kleinberg parameters\n",
        "    print(\"Creating CNNBCN model with NetworkX Kleinberg networks (36 nodes each)...\")\n",
        "    model = CNNBCNModel(\n",
        "        num_classes=10,\n",
        "        num_modules=5,\n",
        "        kleinberg_p=2,  # Distance decay parameter\n",
        "        simple_mode=True  # 78 channels\n",
        "    )\n",
        "\n",
        "    # Print model information\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Nodes per module: {model.num_nodes}\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    train_losses, train_accuracies, test_accuracies = train_model(\n",
        "        model, trainloader, testloader, num_epochs=15, lr=0.001\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Best Test Accuracy: {max(test_accuracies):.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}